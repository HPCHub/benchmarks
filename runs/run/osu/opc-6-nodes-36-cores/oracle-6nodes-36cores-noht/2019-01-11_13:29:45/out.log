NNODE=hpc-lhr2-02-rdma hpc-lhr2-03-rdma hpc-lhr2-04-rdma hpc-lhr2-05-rdma hpc-lhr2-06-rdma hpc-lhr2-07-rdma
NCPU=216
LOG_PPN=1 2 4 8 16 32 36
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/pt2pt/osu_latency -x 10000 -i 100000 -m 131072 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_latency.2.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 2 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/pt2pt/osu_latency -x 10000 -i 100000 -m 131072
# OSU MPI Latency Test v5.4.0
# Size          Latency (us)
0                       1.48
1                       1.49
2                       1.49
4                       1.49
8                       1.49
16                      1.49
32                      1.51
64                      1.55
128                     1.99
256                     2.05
512                     2.14
1024                    2.36
2048                    2.77
4096                    3.65
8192                    5.08
16384                   6.85
32768                   8.99
65536                  12.53
131072                 19.26
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/pt2pt/osu_mbw_mr -V -m 2097152 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_mbw_mr.2.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 2 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/pt2pt/osu_mbw_mr -V -m 2097152
# OSU MPI Multiple Bandwidth / Message Rate Test v5.4.0
# [ pairs: 1 ] [ window size: varied ]

# Uni-directional Bandwidth (MB/sec)
#                 8          16          32          64         128         256         512
1              1.77        2.74        3.87        5.09        5.81        5.96        5.60
2              3.70        5.76        8.15       10.19       11.63       11.92       11.22
4              7.47       11.50       16.28       20.34       23.15       23.89       22.42
8             15.05       23.36       32.35       40.60       46.46       47.77       44.89
16            29.56       47.09       65.98       81.04       92.68       95.56       89.94
32            59.46       94.36      131.72      163.49      184.91      190.99      179.39
64           121.22      185.39      254.72      312.43      352.02      360.85      341.29
128          189.50      298.61      416.48      521.12      590.75      602.88      593.84
256          370.85      582.38      805.17     1002.11     1143.13     1158.65     1102.84
512          698.91     1096.06     1535.28     1908.35     2169.47     2186.05     2016.71
1024        1277.27     1897.31     2617.73     3280.02     3765.96     3899.94     3771.35
2048        2217.79     3267.99     4304.57     5484.82     6576.33     6909.06     6487.66
4096        3318.02     4894.29     6595.31     8211.14     9580.91    10380.98    10726.13
8192        4786.57     6535.91     8322.42     9718.44    10507.41     8480.50     8127.71
16384       6378.22     7314.94     7985.83     8384.92     8744.30     9253.36    10051.86
32768       7796.18     8959.52     9713.10    10160.83    10579.47    11069.93    11294.67
65536      10035.80    11029.13    11579.53    11876.73    12014.28    12018.10    12033.58
131072     10932.91    11526.69    11833.57    11975.47    12052.81    12072.57    12076.98
262144     11528.65    11832.86    12000.61    12082.31    12135.10    12143.95    12150.87
524288     11848.68    12020.93    12093.85    12138.88    12159.85    12162.67    12170.75
1048576    12016.55    12120.46    12158.83    12184.02    12195.55    12197.69    12195.89
2097152    12099.40    12162.86    12198.48    12208.39    12209.82    12214.09    12206.35

# Message Rate Profile
#                 8          16          32          64         128         256         512
1        1771961.80  2743757.09  3869636.77  5094312.44  5806284.21  5957356.50  5596703.10
2        1850810.77  2878225.86  4076827.82  5097209.36  5812929.04  5959823.80  5607962.26
4        1867526.98  2874404.69  4069305.36  5084934.30  5786532.11  5973417.36  5605410.01
8        1880697.92  2919963.79  4043528.59  5075087.51  5807000.69  5971746.64  5611286.66
16       1847741.49  2943059.16  4123737.92  5064769.71  5792216.89  5972745.62  5621022.54
32       1858144.59  2948651.08  4116290.35  5109013.58  5778415.81  5968378.78  5605908.98
64       1894078.41  2896745.52  3979955.94  4881669.10  5500273.51  5638220.09  5332652.31
128      1480469.82  2332858.01  3253712.29  4071277.90  4615198.79  4710038.27  4639383.53
256      1448648.14  2274927.87  3145182.61  3914497.59  4465335.39  4525972.45  4307987.81
512      1365061.93  2140737.43  2998599.09  3727247.06  4237243.25  4269638.34  3938882.70
1024     1247329.94  1852842.61  2556380.17  3203145.49  3677692.75  3808539.94  3682961.47
2048     1082905.93  1595699.59  2101839.37  2678133.54  3211100.78  3373563.07  3167804.61
4096      810063.42  1194895.70  1610181.99  2004672.14  2339089.81  2534420.10  2618684.46
8192      584298.16   797840.25  1015920.43  1186333.29  1282643.01  1035217.74   992151.73
16384     389295.35   446468.81   487416.43   511774.90   533709.93   564780.39   613517.14
32768     237920.62   273422.85   296420.33   310083.84   322859.73   337827.52   344685.91
65536     153134.11   168291.20   176689.62   181224.46   183323.42   183381.67   183617.82
131072     83411.51    87941.67    90282.97    91365.59    91955.63    92106.38    92140.08
262144     43978.33    45138.79    45778.68    46090.36    46291.75    46325.50    46351.88
524288     22599.57    22928.10    23067.19    23153.09    23193.08    23198.44    23213.86
1048576    11459.88    11558.98    11595.56    11619.58    11630.58    11632.62    11630.91
2097152     5769.44     5799.70     5816.69     5821.41     5822.09     5824.13     5820.44
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/pt2pt/osu_mbw_mr -V -m 2097152 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_mbw_mr.2.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
hpc-lhr2-03-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 4 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/pt2pt/osu_mbw_mr -V -m 2097152
# OSU MPI Multiple Bandwidth / Message Rate Test v5.4.0
# [ pairs: 2 ] [ window size: varied ]

# Uni-directional Bandwidth (MB/sec)
#                 8          16          32          64         128         256         512
1              3.24        4.93        6.81        8.68        9.80       10.09        9.62
2              6.65       10.12       14.11       17.35       19.59       20.15       19.25
4             13.58       20.25       28.19       34.71       39.16       40.36       38.50
8             27.17       41.32       55.96       69.19       78.38       80.75       76.95
16            53.76       83.04      114.26      138.39      156.57      161.55      153.94
32           108.11      166.26      227.39      278.87      312.71      322.88      307.72
64           204.17      305.72      414.22      513.80      584.57      606.91      583.16
128          335.30      515.75      718.30      897.99     1021.15     1051.41     1041.57
256          649.70      991.12     1378.06     1727.74     1977.53     2027.55     1952.65
512         1231.66     1854.31     2555.49     3189.21     3655.86     3805.66     3585.46
1024        2253.88     3219.11     4326.11     5339.61     6120.03     6604.00     6575.11
2048        3683.59     5240.67     6649.02     8146.92     9345.63    10066.88    10031.05
4096        5543.27     7515.77     9372.67    10697.40    11398.60    11718.44    11821.07
8192        7827.15     9761.83    10983.81    11770.34    12002.42    12034.53    12050.85
16384       9738.06    11045.14    11784.06    11963.49    12049.55    12061.56    12125.93
32768      11730.62    12063.60    12131.97    12199.85    12246.13    12235.00    12250.55
65536      12144.79    12216.70    12225.89    12163.18    12215.46    12218.60    12221.67
131072     12192.83    12257.59    12243.62    12230.58    12237.62    12236.42    12238.02
262144     11964.90    12123.13    12220.79    12258.54    12253.26    12248.15    12250.63
524288     12191.39    12252.15    12255.92    12251.87    12251.65    12251.16    12252.50
1048576    12189.26    12228.76    12253.82    12252.06    12252.96    12252.51    12251.35
2097152    12231.47    12253.55    12253.19    12254.10    12253.96    12253.48    12253.57

# Message Rate Profile
#                 8          16          32          64         128         256         512
1        3235068.14  4931525.02  6813026.07  8684189.28  9802213.54  10086038.63  9616054.15
2        3324081.30  5059868.52  7052827.33  8676292.62  9794042.01  10077414.03  9627288.10
4        3395110.17  5061265.03  7046669.32  8677942.57  9789225.73  10088832.96  9624823.62
8        3396367.80  5165439.34  6995288.56  8649311.96  9797661.08  10093279.05  9618177.15
16       3360204.48  5190028.01  7141486.25  8649095.72  9785839.20  10097090.83  9621552.58
32       3378339.15  5195741.43  7106026.34  8714623.64  9772105.72  10090106.42  9616363.89
64       3190145.65  4776818.09  6472167.90  8028201.57  9133910.80  9482917.97  9111827.71
128      2619532.55  4029315.79  5611731.32  7015582.27  7977770.69  8214151.93  8137238.99
256      2537908.01  3871575.01  5383059.36  6748995.62  7724721.12  7920099.81  7627558.25
512      2405584.56  3621697.82  4991187.43  6228933.25  7140347.86  7432926.91  7002853.08
1024     2201056.92  3143657.80  4224720.82  5214458.88  5976589.28  6449216.39  6421007.53
2048     1798629.89  2558919.11  3246590.20  3977988.05  4563294.09  4915466.69  4897971.87
4096     1353338.50  1834905.00  2288248.52  2611669.53  2782862.18  2860946.46  2886004.39
8192      955462.32  1191629.25  1340797.10  1436809.11  1465139.20  1469059.22  1471050.91
16384     594363.80   674141.70   719242.06   730193.42   735446.37   736179.05   740107.88
32768     357989.97   368151.77   370238.29   372309.79   373722.35   373382.48   373857.01
65536     185314.78   186412.11   186552.33   185595.44   186393.20   186441.10   186487.96
131072     93023.95    93518.02    93411.37    93311.92    93365.62    93356.51    93368.68
262144     45642.45    46246.09    46618.62    46762.60    46742.49    46722.98    46732.46
524288     23253.23    23369.12    23376.32    23368.58    23368.17    23367.24    23369.79
1048576    11624.59    11662.25    11686.15    11684.47    11685.34    11684.90    11683.79
2097152     5832.42     5842.95     5842.78     5843.21     5843.14     5842.91     5842.96
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/pt2pt/osu_mbw_mr -V -m 2097152 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_mbw_mr.2.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
hpc-lhr2-03-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 8 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/pt2pt/osu_mbw_mr -V -m 2097152
# OSU MPI Multiple Bandwidth / Message Rate Test v5.4.0
# [ pairs: 4 ] [ window size: varied ]

# Uni-directional Bandwidth (MB/sec)
#                 8          16          32          64         128         256         512
1              6.37        9.31       13.37       17.37       19.67       20.28       19.41
2             13.33       20.24       28.07       34.76       39.19       40.69       38.87
4             26.86       40.39       56.25       69.43       78.78       81.45       77.69
8             53.89       82.27      111.67      138.98      157.94      162.93      155.27
16           106.46      165.28      226.47      277.84      315.31      325.91      310.90
32           214.19      329.66      453.71      558.27      628.21      650.72      621.33
64           396.77      588.64      804.60     1015.47     1164.51     1216.02     1170.96
128          661.84     1015.57     1420.48     1782.20     2040.42     2105.84     2091.63
256         1273.02     1952.63     2726.76     3430.31     3935.65     4054.50     3915.51
512         2415.26     3609.10     4944.78     6222.68     7168.01     7539.04     7154.72
1024        4306.84     6090.01     7809.72     9444.43    10824.34    11354.49    11389.64
2048        6887.43     9157.58    11493.05    11949.13    11932.37    11946.76    11955.41
4096       10032.34    11849.61    11977.03    11990.12    11976.72    11988.88    12026.18
8192       12035.68    12049.95    12073.22    12103.45    12082.24    12076.33    12097.33
16384      12140.32    12148.92    12178.04    12159.12    12158.50    12149.43    12165.38
32768      12182.10    12351.80    12388.88    12452.42    12479.51    12481.95    12491.84
65536      12242.17    12252.86    12254.21    12242.20    12243.66    12242.91    12238.68
131072     12247.07    12254.09    12250.32    12244.89    12240.23    12240.90    12241.33
262144     12273.35    12275.05    12269.06    12263.75    12253.13    12251.14    12248.47
524288     12265.76    12272.51    12256.81    12259.21    12255.19    12252.67    12251.95
1048576    12261.02    12258.37    12261.17    12255.98    12253.99    12253.54    11492.13
2097152    12259.21    12260.23    12255.88    12254.34    12253.68    12253.72    11682.57

# Message Rate Profile
#                 8          16          32          64         128         256         512
1        6365115.98  9313866.39  13372353.44  17365427.27  19670870.23  20283828.00  19413837.66
2        6665236.42  10118873.05  14035699.44  17381760.30  19592986.33  20343675.00  19433886.06
4        6715131.02  10098101.48  14062391.43  17358559.54  19694612.34  20362516.42  19422447.26
8        6736657.73  10283437.25  13959028.62  17373110.97  19741964.42  20365776.50  19408517.38
16       6653681.61  10329694.80  14154105.59  17365206.41  19707031.12  20369532.96  19431053.90
32       6693520.94  10301931.09  14178465.89  17445893.87  19631700.47  20334892.24  19416495.44
64       6199456.87  9197566.22  12571932.58  15866768.72  18195471.25  19000297.67  18296259.14
128      5170593.22  7934158.87  11097528.19  13923474.73  15940778.76  16451873.85  16340827.53
256      4972719.97  7627469.63  10651421.82  13399649.39  15373645.27  15837884.40  15294953.15
512      4717304.47  7049017.10  9657769.76  12153680.93  14000024.34  14724695.82  13974062.76
1024     4205894.29  5947278.31  7626677.70  9223075.67  10570642.29  11088370.09  11122698.16
2048     3363001.31  4471472.79  5611839.58  5834536.75  5826350.30  5833379.35  5837604.12
4096     2449302.31  2892971.47  2924079.14  2927274.69  2924004.24  2926972.34  2936077.97
8192     1469199.21  1470941.27  1473782.04  1477471.55  1474882.86  1474161.35  1476725.40
16384     740986.06   741511.00   743288.59   742133.63   742096.22   741542.52   742515.88
32768     371768.21   376946.99   378078.53   380017.74   380844.38   380918.84   381220.74
65536     186800.66   186963.78   186984.33   186801.20   186823.38   186811.97   186747.50
131072     93437.74    93491.27    93462.51    93421.10    93385.51    93390.68    93393.92
262144     46819.10    46825.61    46802.75    46782.50    46741.99    46734.37    46724.19
524288     23395.08    23407.96    23378.01    23382.58    23374.93    23370.12    23368.75
1048576    11693.02    11690.49    11693.16    11688.21    11686.32    11685.88    10959.75
2097152     5845.65     5846.13     5844.06     5843.32     5843.01     5843.03     5570.68
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/pt2pt/osu_mbw_mr -V -m 2097152 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_mbw_mr.2.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
hpc-lhr2-03-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 16 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/pt2pt/osu_mbw_mr -V -m 2097152
# OSU MPI Multiple Bandwidth / Message Rate Test v5.4.0
# [ pairs: 8 ] [ window size: varied ]

# Uni-directional Bandwidth (MB/sec)
#                 8          16          32          64         128         256         512
1             12.18       18.13       25.68       34.53       38.95       40.27       38.50
2             26.09       39.93       55.77       68.98       77.97       80.55       77.11
4             52.80       79.85      111.37      137.68      155.81      160.16      154.18
8            106.25      163.46      221.95      275.68      312.77      322.74      308.34
16           211.23      326.72      449.94      549.83      624.43      645.04      616.63
32           422.95      651.13      894.71     1106.77     1242.55     1287.66     1231.09
64           776.93     1148.77     1589.00     2004.96     2309.89     2406.36     2328.55
128         1283.32     1968.69     2768.72     3494.51     3992.59     4136.78     4114.80
256         2475.17     3753.73     5226.07     6580.01     7510.34     7850.07     7646.02
512         4611.74     6591.63     8349.32     9849.14    10437.35     9938.62     9586.55
1024        7617.73    10126.95    11440.31    11469.71    11462.10    11427.80    11389.78
2048       11255.97    11882.24    11970.66    11964.51    11954.49    11937.16    11957.84
4096       11882.08    12031.57    12026.36    12204.94    11992.68    11987.09    11971.10
8192       12102.70    12068.73    12073.91    12065.32    12058.06    12068.35    12076.63
16384      12182.12    12151.57    12148.60    12149.26    12144.80    12151.82    12156.65
32768      12356.80    12453.77    12486.76    12511.79    12497.05    12514.98    12514.23
65536      12235.67    12232.08    12226.34    12228.25    12227.04    12227.81    12226.22
131072     12245.04    12242.65    12242.61    12241.06    12240.92    12240.79    12240.62
262144     12252.44    12249.76    12249.61    12247.71    12247.81    12247.84    12247.40
524288     12256.30    12251.92    12252.06    12251.00    12251.33    12251.09    12250.75
1048576    12253.83    12253.53    12252.93    12252.82    12252.83    12252.88    12252.81
2097152    12254.68    12253.95    12253.95    12253.28    12253.71    12253.72    12253.56

# Message Rate Profile
#                 8          16          32          64         128         256         512
1        12179025.00  18129477.03  25677600.91  34530622.56  38952441.79  40268958.49  38495498.46
2        13043412.92  19965037.77  27883471.60  34491739.87  38982646.20  40275195.81  38557445.11
4        13200584.95  19962177.11  27841339.17  34419204.91  38953164.14  40039379.55  38545226.82
8        13281745.51  20432335.44  27743756.61  34459454.76  39096399.73  40342512.46  38542749.54
16       13201908.98  20420201.47  28121141.15  34364125.65  39026611.91  40314745.37  38539458.04
32       13217067.85  20347816.51  27959647.45  34586689.90  38829791.79  40239322.79  38471480.01
64       12139573.80  17949475.72  24828082.48  31327508.52  36092080.63  37599298.55  36383601.07
128      10025908.75  15380395.94  21630587.16  27300858.87  31192109.74  32318623.12  32146872.28
256      9668613.92  14663006.46  20414329.16  25703145.12  29337248.21  30664326.61  29867284.82
512      9007313.16  12874279.77  16307261.94  19236604.55  20385442.64  19411364.36  18723734.42
1024     7439187.55  9889596.76  11172179.88  11200889.60  11193457.45  11159960.69  11122829.81
2048     5496077.93  5801876.57  5845050.62  5842046.33  5837152.36  5828689.54  5838790.21
4096     2900898.18  2937395.89  2936122.46  2979722.67  2927901.32  2926536.07  2922630.72
8192     1477380.68  1473233.27  1473866.45  1472817.64  1471931.55  1473186.97  1474197.63
16384     743537.32   741672.95   741491.71   741532.06   741259.67   741688.25   741982.74
32768     377099.69   380058.76   381065.56   381829.64   381379.67   381926.97   381903.89
65536     186701.53   186646.73   186559.17   186588.34   186569.81   186581.54   186557.27
131072     93422.25    93403.98    93403.68    93391.89    93390.83    93389.80    93388.53
262144     46739.37    46729.12    46728.56    46721.29    46721.67    46721.82    46720.14
524288     23377.05    23368.69    23368.95    23366.92    23367.56    23367.10    23366.45
1048576    11686.16    11685.87    11685.30    11685.20    11685.20    11685.26    11685.19
2097152     5843.48     5843.14     5843.14     5842.82     5843.03     5843.03     5842.95
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/pt2pt/osu_mbw_mr -V -m 2097152 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_mbw_mr.2.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
hpc-lhr2-03-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 32 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/pt2pt/osu_mbw_mr -V -m 2097152
# OSU MPI Multiple Bandwidth / Message Rate Test v5.4.0
# [ pairs: 16 ] [ window size: varied ]

# Uni-directional Bandwidth (MB/sec)
#                 8          16          32          64         128         256         512
1             23.81       35.34       50.27       66.92       75.53       77.93       74.77
2             51.56       78.44      108.41      134.26      150.65      156.43      150.08
4            103.76      156.25      217.42      267.17      303.45      313.37      300.06
8            209.28      319.12      433.14      536.20      607.69      626.68      599.84
16           415.48      639.00      873.11     1066.16     1210.80     1252.20     1199.01
32           825.57     1271.09     1741.04     2136.98     2413.78     2497.73     2394.85
64          1513.94     2233.76     3051.27     3844.65     4345.50     4534.51     4466.89
128         2442.75     3656.56     4832.31     5904.24     6415.94     5512.74     5313.52
256         4604.88     6762.85     8528.39     8694.13     7946.79     7674.66     7538.17
512         8070.62    10123.89    10459.96    10525.66     9687.21     9508.72     9420.46
1024       11311.40    11444.88    11461.05    11424.46    11416.14    11337.93    11299.04
2048       11912.65    11942.80    11958.76    11951.30    11947.22    11908.74    11929.91
4096       12016.88    12009.89    11966.85    11962.05    11962.94    11947.94    11976.18
8192       12089.24    12066.69    12023.50    12016.63    12006.48    12051.35    12056.53
16384      12154.59    12139.55    12107.57    12097.33    12100.83    12141.26    12140.86
32768      12180.64    12171.53    12164.89    12135.95    12174.14    12176.53    12183.96
65536      12224.66    12227.73    12225.03    12225.34    12224.95    12225.32    12225.42
131072     12220.43    12239.15    12240.09    12239.79    12239.80    12240.08    12239.75
262144     12247.82    12248.48    12247.26    12247.31    12247.25    12247.35    12246.79
524288     12250.90    12252.18    12249.79    12250.95    12250.97    12250.95    12250.88
1048576    12253.07    12252.73    12252.82    12252.51    12252.61    12252.79    12252.81
2097152    12253.72    12252.90    12253.76    12253.54    12253.70    12253.67    12005.27

# Message Rate Profile
#                 8          16          32          64         128         256         512
1        23810249.07  35338118.75  50266315.75  66923323.19  75525652.73  77926733.21  74771883.63
2        25778830.94  39217987.93  54202582.55  67130348.48  75324406.67  78216915.92  75037732.84
4        25940803.78  39061989.65  54355539.53  66792699.28  75862897.65  78343641.17  75013961.53
8        26159619.67  39890033.67  54142452.80  67024740.12  75961456.16  78335495.05  74980081.27
16       25967455.74  39937468.98  54569088.72  66634821.54  75674772.87  78262359.70  74937911.17
32       25799208.59  39721553.45  54407408.58  66780666.05  75430745.74  78054084.62  74838908.95
64       23655363.51  34902422.78  47676129.18  60072660.64  67898371.75  70851738.85  69795100.49
128      19083965.92  28566851.40  37752418.59  46126900.11  50124498.22  43068319.40  41511880.47
256      17987808.87  26417397.19  33314018.09  33961457.94  31042151.08  29979147.81  29445987.20
512      15762931.67  19773218.19  20429609.85  20557928.42  18920333.31  18571717.23  18399328.73
1024     11046284.30  11176642.91  11192431.42  11156695.84  11148569.42  11072201.45  11034218.20
2048     5816721.36  5831447.53  5839237.33  5835595.27  5833603.99  5814813.22  5825151.67
4096     2933809.63  2932101.37  2921594.46  2920421.79  2920639.12  2916976.95  2923872.83
8192     1475737.57  1472985.02  1467712.17  1466873.21  1465634.48  1471112.42  1471744.18
16384     741857.53   740939.17   738987.16   738362.39   738576.02   741043.71   741019.43
32768     371723.56   371445.54   371243.08   370359.68   371525.22   371598.18   371824.81
65536     186533.55   186580.38   186539.12   186543.91   186537.87   186543.58   186545.12
131072     93234.51    93377.28    93384.51    93382.19    93382.25    93384.42    93381.86
262144     46721.73    46724.25    46719.60    46719.80    46719.55    46719.94    46717.80
524288     23366.75    23369.18    23364.63    23366.83    23366.87    23366.84    23366.71
1048576    11685.44    11685.11    11685.20    11684.91    11685.00    11685.18    11685.19
2097152     5843.03     5842.64     5843.05     5842.94     5843.02     5843.01     5724.56
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/pt2pt/osu_mbw_mr -V -m 2097152 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_mbw_mr.2.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
hpc-lhr2-03-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 64 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/pt2pt/osu_mbw_mr -V -m 2097152
# OSU MPI Multiple Bandwidth / Message Rate Test v5.4.0
# [ pairs: 32 ] [ window size: varied ]

# Uni-directional Bandwidth (MB/sec)
#                 8          16          32          64         128         256         512
1             46.54       70.28       98.36      121.14      131.56      125.86      132.61
2            100.72      152.25      208.53      243.84      262.85      265.07      265.22
4            203.11      305.17      413.83      463.84      506.23      510.02      508.11
8            403.93      621.79      767.26      897.57      981.00      983.31      975.86
16           804.72     1224.62     1470.45     1716.53     1817.12     1789.81     1776.95
32          1568.91     2161.14     2727.75     3175.12     3060.72     3024.98     3027.46
64          2916.16     4140.04     4551.81     4591.36     4070.31     3055.62     2897.00
128         3832.45     4849.77     5355.50     5048.93     5082.88     5140.48     5143.79
256         6877.24     8152.24     8335.53     7762.52     7593.92     7520.55     7483.25
512         9892.65    10317.99    10382.90     9646.30     9478.07     9419.46     9338.08
1024       11363.46    10910.22    11485.33    11381.99    11083.36    10994.36    11092.20
2048       11917.56    11935.29    11937.20    11904.02    11804.97    11645.60    11834.42
4096       11951.42    11966.32    11822.78    11893.98    11796.84    11799.67    11881.72
8192       12066.25    12042.28    11949.00    11938.53    11889.52    11966.07    12013.87
16384      12048.11    12074.28    12048.00    12039.64    12071.01    12092.65    12112.26
32768      12173.91    12096.68    12087.67    12115.31    12129.09    12161.82    12157.74
65536      12226.42    12225.16    12220.72    12214.27    12207.70    12211.74    12212.75
131072     12239.86    12240.32    12238.83    12234.07    12235.45    12232.23    12231.10
262144     12246.48    12244.59    12246.31    12244.03    12242.27    12243.28    12243.24
524288     12250.75    12250.98    12250.59    12249.65    12248.95    12249.05    12249.21
1048576    12252.58    12252.29    12252.79    12251.93    12251.71    12251.85    12252.08
2097152    12253.71    12253.73    12253.60    12253.33    12253.21    12042.97    12253.31

# Message Rate Profile
#                 8          16          32          64         128         256         512
1        46539322.45  70280842.40  98363083.75  121141128.89  131558800.43  125856742.63  132605798.90
2        50359239.89  76126043.04  104267247.22  121920845.64  131424039.01  132534081.53  132608032.99
4        50778673.48  76292948.66  103458467.45  115960096.45  126556991.99  127504602.90  127026316.30
8        50491475.21  77723380.49  95907146.16  112196279.68  122625393.67  122913242.65  121982344.69
16       50294902.37  76538905.41  91902853.21  107283339.82  113570222.28  111862963.98  111059067.00
32       49028520.73  67535724.70  85242102.08  99222443.10  95647510.66  94530778.34  94608036.30
64       45565014.51  64688063.66  71122055.69  71740006.92  63598603.68  47744027.15  45265691.49
128      29941014.88  37888794.02  41839850.74  39444766.52  39709973.23  40159977.79  40185838.78
256      26864232.45  31844674.50  32560649.55  30322354.58  29663768.43  29377157.23  29231447.09
512      19321590.93  20152332.65  20279100.53  18840431.58  18511847.27  18397388.16  18238443.06
1024     11097128.17  10654515.18  11216140.38  11115221.05  10823590.66  10736681.75  10832228.74
2048     5819119.99  5827777.00  5828709.24  5812509.79  5764143.66  5686330.05  5778523.84
4096     2917827.34  2921464.64  2886419.82  2903804.58  2880089.05  2880778.09  2900809.95
8192     1472931.39  1470005.08  1458617.98  1457339.99  1451357.15  1460701.69  1466537.48
16384     735358.18   736955.77   735351.58   734841.00   736756.15   738076.74   739273.72
32768     371518.38   369161.34   368886.28   369729.79   370150.44   371149.30   371024.66
65536     186560.34   186541.13   186473.46   186374.93   186274.77   186336.32   186351.84
131072     93382.70    93386.21    93374.84    93338.56    93349.10    93324.48    93315.92
262144     46716.60    46709.42    46715.97    46707.28    46700.57    46704.40    46704.26
524288     23366.45    23366.90    23366.15    23364.35    23363.01    23363.20    23363.52
1048576    11684.97    11684.70    11685.17    11684.35    11684.14    11684.28    11684.49
2097152     5843.03     5843.03     5842.97     5842.84     5842.79     5742.54     5842.83
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/pt2pt/osu_mbw_mr -V -m 2097152 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_mbw_mr.2.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
hpc-lhr2-03-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 72 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/pt2pt/osu_mbw_mr -V -m 2097152
# OSU MPI Multiple Bandwidth / Message Rate Test v5.4.0
# [ pairs: 36 ] [ window size: varied ]

# Uni-directional Bandwidth (MB/sec)
#                 8          16          32          64         128         256         512
1             50.51       73.42      100.21       95.05       99.88       98.57       96.90
2            105.38      151.94      187.73      200.79      183.24      192.29      195.64
4            211.24      301.69      373.64      397.05      362.89      382.83      382.84
8            422.44      603.42      747.20      792.12      733.29      762.62      772.32
16           841.52     1201.35     1470.45     1578.22     1459.71     1503.19     1546.27
32          1666.49     2333.11     2767.36     3038.25     2834.36     2893.37     2806.12
64          3030.34     4052.91     4490.00     4058.15     3039.05     2833.47     2741.35
128         3984.76     5015.12     5292.58     4915.38     4937.32     4920.99     4852.08
256         7274.94     8155.32     8509.53     7571.94     7418.76     7297.75     7243.18
512         9747.75    10302.10     9973.65     9406.05     9350.85     9243.65     9249.08
1024       11378.78    11422.72    11388.31    11155.87    11030.12    10930.29    10882.00
2048       11951.53    12002.10    11964.38    11955.50    11918.23    11766.72    10747.48
4096       11097.75    10478.23    10382.48    10799.23    10227.32    10317.10    10299.21
8192       10520.99    10388.90    10468.13    10529.12    10371.58    10484.39    10473.69
16384      10691.29    10598.31    10629.15    10568.78    10623.86    10547.49    10597.14
32768      10699.51    10488.95    10615.45    10625.55    10670.04    10673.35    10657.80
65536      11393.63    11431.47    11184.43    11343.09    11456.54    11485.77    11500.42
131072     11820.01    11722.13    11809.62    11765.68    11804.69    11801.25    11813.37
262144     12030.72    11982.72    11958.87    11966.27    11978.11    11991.94    11993.92
524288     12128.33    12091.15    12113.93    12101.45    12101.26    12102.94    12107.73
1048576    12169.08    12158.30    12157.08    12150.19    12157.41    12164.32    12011.26
2097152    12178.36    12184.57    12186.63    12188.51    12192.04    12193.12    12066.60

# Message Rate Profile
#                 8          16          32          64         128         256         512
1        50514415.84  73418719.86  100209504.42  95053828.26  99882633.93  98574756.38  96896191.50
2        52689770.07  75968076.37  93866885.89  100393485.01  91618407.46  96146966.59  97817789.21
4        52810713.71  75421430.33  93410240.02  99261612.51  90723712.09  95707288.84  95710411.75
8        52805326.26  75427935.12  93399694.09  99014927.77  91661352.07  95327554.85  96540130.86
16       52595257.40  75084116.60  91903393.48  98638857.75  91231938.55  93949265.33  96641886.12
32       52077742.70  72909764.45  86480123.87  94945212.54  88573860.89  90417911.57  87691273.64
64       47349064.34  63326702.30  70156197.68  63408629.40  47485131.88  44272970.43  42833664.97
128      31130960.04  39180589.07  41348271.50  38401407.52  38572846.71  38445214.24  37906866.78
256      28417721.65  31856723.89  33240363.12  29577881.97  28979535.87  28506836.98  28293685.78
512      19038565.31  20121286.45  19479776.67  18371200.63  18263379.98  18054008.86  18064606.31
1024     11112087.13  11154996.13  11121400.18  10894405.44  10771597.16  10674111.69  10626949.37
2048     5835706.42  5860399.92  5841980.50  5837645.41  5819448.03  5745470.89  5247794.83
4096     2709412.31  2558162.78  2534785.41  2636531.11  2496905.35  2518822.50  2514455.74
8192     1284301.07  1268175.74  1277848.02  1285293.40  1266061.46  1279832.60  1278526.89
16384     652544.44   646869.67   648751.93   645067.17   648428.93   643767.82   646798.33
32768     326523.27   320097.50   323957.76   324265.98   325623.78   325724.91   325250.10
65536     173853.01   174430.39   170660.85   173081.86   174812.86   175258.91   175482.42
131072     90179.54    89432.77    90100.29    89764.99    90062.64    90036.39    90128.88
262144     45893.55    45710.45    45619.46    45647.71    45692.86    45745.61    45753.18
524288     23132.96    23062.04    23105.48    23081.68    23081.31    23084.52    23093.65
1048576    11605.34    11595.06    11593.90    11587.32    11594.21    11600.80    11454.83
2097152     5807.09     5810.06     5811.04     5811.94     5813.62     5814.13     5753.80
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.1.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 2 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       0.56
2                       0.57
4                       0.56
8                       0.56
16                      0.56
32                      0.82
64                      0.96
128                     1.01
256                     1.05
512                     1.10
1024                    1.29
2048                    1.58
4096                    2.18
8192                    4.09
16384                   6.66
32768                  12.24
65536                  23.25
131072                 53.30
262144                 33.53
524288                107.84
1048576               239.73
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.1.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 2 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             0.53
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.1.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 2 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       0.62
8                       0.63
16                      0.64
32                      0.89
64                      0.90
128                     1.12
256                     1.22
512                     1.28
1024                    1.48
2048                    2.10
4096                    2.81
8192                    5.12
16384                   8.77
32768                  15.40
65536                  29.34
131072                 60.44
262144                135.05
524288                258.43
1048576               548.71
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.1.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 4 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       1.33
2                       1.34
4                       1.34
8                       1.34
16                      1.34
32                      1.69
64                      1.73
128                     2.03
256                     2.38
512                     2.57
1024                    3.16
2048                    3.97
4096                    6.73
8192                   11.11
16384                  17.94
32768                  32.24
65536                  59.03
131072                139.25
262144                107.88
524288                241.31
1048576               492.58
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.1.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 4 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             1.06
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.1.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 4 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       1.14
8                       1.15
16                      1.15
32                      1.46
64                      1.51
128                     1.83
256                     2.02
512                     2.19
1024                    2.68
2048                    3.47
4096                    4.83
8192                    8.59
16384                  12.65
32768                  22.53
65536                  37.91
131072                 74.90
262144                159.60
524288                327.46
1048576               788.09
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.1.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 8 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       2.89
2                       2.86
4                       2.94
8                       2.91
16                      2.91
32                      3.49
64                      3.56
128                     4.31
256                     4.92
512                     5.62
1024                    7.28
2048                    9.01
4096                   16.24
8192                   28.22
16384                  45.96
32768                  82.57
65536                 153.99
131072                329.92
262144                293.29
524288               1092.34
1048576              3038.43
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.1.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 8 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             1.21
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.1.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 8 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       1.34
8                       1.36
16                      1.38
32                      1.70
64                      1.83
128                     2.31
256                     2.54
512                     2.81
1024                    3.60
2048                    4.81
4096                    6.98
8192                   12.57
16384                  18.82
32768                  28.42
65536                  54.01
131072                102.39
262144                197.77
524288                390.99
1048576               785.04
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.1.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 16 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       5.79
2                       5.82
4                       5.84
8                       6.28
16                      6.87
32                      6.71
64                      7.05
128                     7.74
256                    10.32
512                    12.05
1024                   16.02
2048                   20.63
4096                   38.26
8192                   73.75
16384                 122.49
32768                 226.32
65536                 446.30
131072                928.76
262144               1641.98
524288               3494.22
1048576              7004.10
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.1.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 16 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             1.84
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.1.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 16 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       1.93
8                       1.95
16                      1.93
32                      2.32
64                      2.51
128                     3.04
256                     3.35
512                     3.69
1024                    4.64
2048                    6.34
4096                    9.33
8192                   17.74
16384                  26.81
32768                  38.28
65536                  60.65
131072                122.63
262144                251.37
524288                461.02
1048576               866.16
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.1.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 32 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       8.11
2                       8.21
4                       8.43
8                       8.67
16                      9.05
32                      9.61
64                     10.77
128                    12.84
256                    22.32
512                    26.30
1024                   38.27
2048                   56.56
4096                  101.40
8192                  192.46
16384                 352.76
32768                 784.61
65536                1646.05
131072               3446.46
262144               4353.80
524288               8527.40
1048576             16940.91
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.1.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 32 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             2.25
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.1.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 32 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       2.36
8                       2.43
16                      2.70
32                      2.92
64                      3.22
128                     3.93
256                     4.01
512                     4.56
1024                    6.62
2048                    9.49
4096                   14.86
8192                   27.82
16384                  38.09
32768                  57.26
65536                  82.65
131072                139.45
262144                287.60
524288                615.87
1048576              1239.47
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.1.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 36 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       9.49
2                       9.82
4                       9.83
8                      10.58
16                     10.61
32                     11.30
64                     12.43
128                    14.76
256                    24.44
512                    29.26
1024                   45.18
2048                   68.63
4096                  121.83
8192                  236.92
16384                 436.77
32768                 997.22
65536                2135.61
131072               4399.30
262144               5196.29
524288              10369.08
1048576             20810.23
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.1.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 36 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             3.37
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.1.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 36 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       2.84
8                       3.90
16                      4.80
32                      3.72
64                      4.11
128                     4.63
256                     5.14
512                     5.64
1024                    7.74
2048                   10.29
4096                   15.48
8192                   30.05
16384                  39.61
32768                  60.37
65536                  90.47
131072                154.75
262144                289.06
524288                625.17
1048576              1362.37
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.2.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 2 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       1.67
2                       1.67
4                       1.61
8                       1.59
16                      1.58
32                      1.61
64                      1.65
128                     2.18
256                     2.30
512                     2.31
1024                    2.50
2048                    2.91
4096                    3.85
8192                    5.39
16384                   7.44
32768                  10.01
65536                  14.38
131072                 31.15
262144                 53.10
524288                103.45
1048576               253.22
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.2.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 2 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             1.54
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.2.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 2 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       1.72
8                       1.69
16                      1.99
32                      1.68
64                      1.72
128                     2.28
256                     2.38
512                     2.51
1024                    2.75
2048                    3.30
4096                    4.53
8192                    6.74
16384                  12.00
32768                  17.25
65536                  25.02
131072                 44.98
262144                 92.20
524288                175.67
1048576               364.56
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.2.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
hpc-lhr2-03-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 4 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       2.75
2                       2.47
4                       2.41
8                       2.40
16                      2.40
32                      2.51
64                      2.64
128                     3.39
256                     3.48
512                     3.62
1024                    4.04
2048                    4.97
4096                   12.12
8192                   15.89
16384                  22.08
32768                  33.37
65536                  59.90
131072                146.08
262144                156.83
524288                304.42
1048576               606.94
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.2.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
hpc-lhr2-03-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 4 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             2.32
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.2.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
hpc-lhr2-03-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 4 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       2.54
8                       2.59
16                      2.51
32                      2.77
64                      2.90
128                     3.66
256                     3.81
512                     4.12
1024                    4.58
2048                    5.63
4096                    7.76
8192                   12.71
16384                  20.51
32768                  31.57
65536                  47.41
131072                 81.84
262144                166.08
524288                343.68
1048576               729.61
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.2.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
hpc-lhr2-03-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 8 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       3.61
2                       3.35
4                       3.23
8                       3.23
16                      3.24
32                      3.37
64                      3.52
128                     4.69
256                     4.98
512                     5.52
1024                    6.70
2048                    7.99
4096                   24.69
8192                   37.83
16384                  54.59
32768                  86.66
65536                 172.08
131072                345.53
262144                447.55
524288                859.62
1048576              1654.77
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.2.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
hpc-lhr2-03-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 8 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             2.91
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.2.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
hpc-lhr2-03-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 8 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       3.16
8                       3.13
16                      3.06
32                      3.43
64                      3.57
128                     4.42
256                     4.63
512                     4.97
1024                    5.69
2048                    7.16
4096                   10.53
8192                   17.47
16384                  24.60
32768                  35.01
65536                  58.92
131072                102.07
262144                195.54
524288                381.38
1048576               773.49
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.2.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
hpc-lhr2-03-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 16 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       9.30
2                       8.92
4                       8.68
8                       8.87
16                      9.15
32                      9.52
64                      9.97
128                    10.83
256                    10.01
512                    11.36
1024                   14.59
2048                   17.52
4096                   58.19
8192                   95.67
16384                 141.60
32768                 254.71
65536                 489.67
131072                968.89
262144               1527.59
524288               3180.95
1048576              6864.97
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.2.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
hpc-lhr2-03-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 16 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             3.18
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.2.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
hpc-lhr2-03-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 16 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       3.57
8                       3.55
16                      3.54
32                      3.87
64                      4.02
128                     5.19
256                     5.37
512                     5.89
1024                    6.97
2048                    9.01
4096                   13.36
8192                   23.90
16384                  32.99
32768                  43.29
65536                  65.34
131072                123.34
262144                235.88
524288                445.81
1048576               852.73
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.2.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
hpc-lhr2-03-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 32 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      11.99
2                      11.56
4                      11.49
8                      11.88
16                     12.20
32                     12.90
64                     13.97
128                    16.33
256                    21.58
512                    27.11
1024                   36.41
2048                   68.06
4096                  229.21
8192                  326.62
16384                 494.61
32768                 890.32
65536                1704.33
131072               3317.07
262144               6015.54
524288              12044.87
1048576             24046.59
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.2.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
hpc-lhr2-03-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 32 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             3.87
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.2.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
hpc-lhr2-03-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 32 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       4.37
8                       4.30
16                      4.41
32                      4.61
64                      4.79
128                     6.27
256                     6.49
512                     7.09
1024                    8.53
2048                   11.68
4096                   17.93
8192                   33.57
16384                  45.28
32768                  60.38
65536                  82.86
131072                136.21
262144                274.80
524288                533.96
1048576              1003.01
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.2.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
hpc-lhr2-03-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 64 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      15.71
2                      15.56
4                      15.77
8                      16.40
16                     17.30
32                     19.21
64                     22.58
128                    32.90
256                    75.39
512                   121.70
1024                  176.16
2048                  349.35
4096                  573.56
8192                  820.36
16384                1633.87
32768                3261.12
65536                6444.18
131072              13213.13
262144              22770.38
524288              45355.77
1048576             90690.29
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.2.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
hpc-lhr2-03-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 64 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             4.44
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.2.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
hpc-lhr2-03-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 64 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       4.83
8                       4.78
16                      4.72
32                      5.33
64                      5.56
128                     7.93
256                     7.60
512                     8.60
1024                   11.24
2048                   15.90
4096                   27.79
8192                   53.18
16384                  72.38
32768                  84.26
65536                 119.43
131072                172.92
262144                302.65
524288                632.34
1048576              1488.13
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.2.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
hpc-lhr2-03-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 72 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      19.19
2                      19.14
4                      19.32
8                      19.84
16                     20.88
32                     22.82
64                     26.77
128                    41.71
256                   111.37
512                   166.68
1024                  224.48
2048                  412.77
4096                  670.25
8192                 1058.48
16384                2563.69
32768                5248.04
65536                9922.60
131072              18270.23
262144              35863.54
524288              64350.90
1048576            121250.72
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.2.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
hpc-lhr2-03-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 72 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             7.64
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.2.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
hpc-lhr2-03-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 72 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       7.85
8                       7.69
16                      7.68
32                      8.36
64                      8.49
128                    10.63
256                    11.08
512                    12.10
1024                   14.63
2048                   19.24
4096                   29.20
8192                   49.82
16384                  86.60
32768                  90.40
65536                 127.34
131072                184.86
262144                318.53
524288                613.49
1048576              1457.40
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.3.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
hpc-lhr2-04-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 3 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       2.02
2                       1.92
4                       1.87
8                       1.84
16                      1.84
32                      1.87
64                      1.93
128                     2.45
256                     2.52
512                     2.63
1024                    2.90
2048                    4.07
4096                    7.69
8192                   10.67
16384                  14.60
32768                  19.43
65536                  27.96
131072                 66.17
262144                 89.24
524288                163.08
1048576               358.58
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.3.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
hpc-lhr2-04-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 3 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             3.07
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.3.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
hpc-lhr2-04-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 3 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       3.56
8                       3.45
16                      3.35
32                      3.38
64                      3.50
128                     4.50
256                     4.65
512                     4.92
1024                    5.48
2048                    6.61
4096                    8.93
8192                   13.39
16384                  19.19
32768                  25.83
65536                  40.17
131072                 66.06
262144                124.77
524288                247.28
1048576               450.75
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.3.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
hpc-lhr2-03-rdma slots=2
hpc-lhr2-04-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 6 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       3.08
2                       2.88
4                       2.82
8                       2.82
16                      2.81
32                      2.89
64                      3.10
128                     4.12
256                     3.99
512                     4.28
1024                    4.95
2048                    6.37
4096                   19.31
8192                   28.07
16384                  38.65
32768                  57.78
65536                 102.59
131072                219.54
262144                276.56
524288                552.35
1048576              1071.56
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.3.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
hpc-lhr2-03-rdma slots=2
hpc-lhr2-04-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 6 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             4.39
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.3.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
hpc-lhr2-03-rdma slots=2
hpc-lhr2-04-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 6 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       4.46
8                       4.40
16                      4.21
32                      4.46
64                      4.76
128                     6.17
256                     6.44
512                     6.75
1024                    7.68
2048                    9.26
4096                   12.40
8192                   19.62
16384                  27.46
32768                  39.29
65536                  60.30
131072                101.03
262144                186.94
524288                373.35
1048576               754.57
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.3.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
hpc-lhr2-03-rdma slots=4
hpc-lhr2-04-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 12 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       4.93
2                       4.53
4                       4.48
8                       4.44
16                      4.45
32                      4.55
64                      4.98
128                     6.42
256                     6.92
512                     7.89
1024                    9.82
2048                   12.04
4096                   44.73
8192                   68.38
16384                  95.80
32768                 155.87
65536                 297.34
131072                609.73
262144               1224.95
524288               2352.40
1048576              4773.86
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.3.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
hpc-lhr2-03-rdma slots=4
hpc-lhr2-04-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 12 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             5.26
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.3.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
hpc-lhr2-03-rdma slots=4
hpc-lhr2-04-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 12 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       4.78
8                       4.69
16                      4.59
32                      5.12
64                      5.49
128                     6.71
256                     6.88
512                     7.22
1024                    8.09
2048                    9.80
4096                   13.58
8192                   22.44
16384                  33.40
32768                  43.65
65536                  66.31
131072                118.65
262144                223.64
524288                423.49
1048576               831.04
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.3.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
hpc-lhr2-03-rdma slots=8
hpc-lhr2-04-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 24 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      12.57
2                      12.14
4                      11.76
8                      12.12
16                     12.70
32                     13.16
64                     13.91
128                    15.07
256                    14.79
512                    18.12
1024                   23.23
2048                   38.60
4096                  157.59
8192                  219.93
16384                 356.28
32768                 650.48
65536                1233.09
131072               2453.43
262144               5353.66
524288              10360.29
1048576             19074.24
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.3.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
hpc-lhr2-03-rdma slots=8
hpc-lhr2-04-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 24 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             6.06
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.3.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
hpc-lhr2-03-rdma slots=8
hpc-lhr2-04-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 24 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       5.40
8                       5.28
16                      5.12
32                      5.65
64                      5.91
128                     7.32
256                     7.84
512                     8.02
1024                    9.22
2048                   11.44
4096                   16.50
8192                   28.46
16384                  42.76
32768                  56.60
65536                  76.77
131072                128.67
262144                256.31
524288                488.22
1048576               926.93
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.3.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
hpc-lhr2-03-rdma slots=16
hpc-lhr2-04-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 48 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      16.00
2                      14.96
4                      15.34
8                      16.08
16                     16.60
32                     17.67
64                     19.06
128                    23.12
256                    42.70
512                    66.79
1024                   87.79
2048                  143.53
4096                  434.16
8192                  793.35
16384                1453.44
32768                2896.66
65536                6035.64
131072              11403.16
262144              22130.80
524288              43300.83
1048576             85429.62
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.3.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
hpc-lhr2-03-rdma slots=16
hpc-lhr2-04-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 48 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             6.87
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.3.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
hpc-lhr2-03-rdma slots=16
hpc-lhr2-04-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 48 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       5.83
8                       6.37
16                      5.60
32                      6.17
64                      6.45
128                     7.96
256                     8.31
512                     8.91
1024                   10.77
2048                   13.95
4096                   20.76
8192                   43.61
16384                  64.50
32768                  75.67
65536                 104.05
131072                150.22
262144                290.35
524288                628.72
1048576              1649.80
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.3.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
hpc-lhr2-03-rdma slots=32
hpc-lhr2-04-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 96 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      20.92
2                      21.09
4                      21.61
8                      22.15
16                     26.88
32                     26.91
64                     32.79
128                    66.28
256                   216.96
512                   295.64
1024                  403.48
2048                  735.27
4096                 1371.24
8192                 2902.02
16384                5634.43
32768               10779.79
65536               19424.96
131072              37365.30
262144              92278.31
524288             193484.38
1048576            387021.33
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.3.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
hpc-lhr2-03-rdma slots=32
hpc-lhr2-04-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 96 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             7.66
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.3.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
hpc-lhr2-03-rdma slots=32
hpc-lhr2-04-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 96 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       6.54
8                       6.46
16                      6.31
32                      7.03
64                      7.40
128                     9.17
256                     9.59
512                    10.61
1024                   13.13
2048                   17.80
4096                   32.32
8192                  145.26
16384                 132.56
32768                 114.07
65536                 141.23
131072                211.84
262144                330.68
524288                632.67
1048576              1375.48
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.3.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
hpc-lhr2-03-rdma slots=36
hpc-lhr2-04-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 108 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      22.52
2                      22.53
4                      23.17
8                      24.09
16                     29.88
32                     29.47
64                     38.15
128                    97.45
256                   325.19
512                   423.67
1024                  575.50
2048                 1019.03
4096                 1782.60
8192                 3796.41
16384                7267.82
32768               13797.14
65536               25068.01
131072              48816.11
262144             116634.80
524288             243098.04
1048576            461775.93
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.3.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
hpc-lhr2-03-rdma slots=36
hpc-lhr2-04-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 108 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             8.21
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.3.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
hpc-lhr2-03-rdma slots=36
hpc-lhr2-04-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 108 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       8.63
8                       8.29
16                      8.12
32                      9.37
64                      9.43
128                    13.27
256                    18.96
512                    15.58
1024                   16.29
2048                   20.89
4096                   31.58
8192                  116.52
16384                 125.08
32768                 123.46
65536                 152.03
131072                231.43
262144                350.20
524288                666.11
1048576              1490.44
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.4.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
hpc-lhr2-04-rdma slots=1
hpc-lhr2-05-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 4 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       2.32
2                       2.14
4                       2.08
8                       2.08
16                      2.07
32                      2.11
64                      2.17
128                     2.71
256                     2.80
512                     2.94
1024                    3.27
2048                    4.43
4096                   11.46
8192                   16.13
16384                  21.69
32768                  29.85
65536                  48.56
131072                109.83
262144                189.27
524288                353.17
1048576               705.89
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.4.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
hpc-lhr2-04-rdma slots=1
hpc-lhr2-05-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 4 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             3.18
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.4.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
hpc-lhr2-04-rdma slots=1
hpc-lhr2-05-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 4 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       3.43
8                       3.36
16                      3.26
32                      3.26
64                      3.36
128                     4.40
256                     4.56
512                     4.85
1024                    5.33
2048                    6.48
4096                    8.91
8192                   12.97
16384                  24.98
32768                  35.78
65536                  50.56
131072                 80.57
262144                144.18
524288                278.40
1048576               511.46
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.4.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
hpc-lhr2-03-rdma slots=2
hpc-lhr2-04-rdma slots=2
hpc-lhr2-05-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 8 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       3.89
2                       3.63
4                       3.58
8                       3.59
16                      3.58
32                      3.63
64                      3.85
128                     4.81
256                     5.02
512                     5.55
1024                    6.38
2048                    7.53
4096                   28.71
8192                   41.73
16384                  56.23
32768                  83.36
65536                 149.96
131072                302.74
262144                400.29
524288                779.76
1048576              1664.12
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.4.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
hpc-lhr2-03-rdma slots=2
hpc-lhr2-04-rdma slots=2
hpc-lhr2-05-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 8 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             4.14
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.4.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
hpc-lhr2-03-rdma slots=2
hpc-lhr2-04-rdma slots=2
hpc-lhr2-05-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 8 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       4.50
8                       4.27
16                      4.24
32                      4.56
64                      4.78
128                     6.08
256                     6.30
512                     6.68
1024                    7.72
2048                    9.12
4096                   12.56
8192                   19.71
16384                  34.93
32768                  47.26
65536                  73.46
131072                116.71
262144                201.97
524288                392.05
1048576               790.97
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.4.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
hpc-lhr2-03-rdma slots=4
hpc-lhr2-04-rdma slots=4
hpc-lhr2-05-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 16 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      10.40
2                      10.06
4                       9.89
8                      10.25
16                     10.57
32                     10.93
64                     11.43
128                    12.28
256                     9.06
512                    10.47
1024                   13.08
2048                   15.68
4096                   66.24
8192                  101.25
16384                 142.50
32768                 229.60
65536                 504.93
131072               1106.64
262144               2133.96
524288               3742.92
1048576              6838.31
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.4.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
hpc-lhr2-03-rdma slots=4
hpc-lhr2-04-rdma slots=4
hpc-lhr2-05-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 16 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             4.69
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.4.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
hpc-lhr2-03-rdma slots=4
hpc-lhr2-04-rdma slots=4
hpc-lhr2-05-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 16 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       5.27
8                       5.02
16                      4.94
32                      5.43
64                      5.63
128                     6.86
256                     7.13
512                     8.19
1024                    8.68
2048                   10.79
4096                   15.28
8192                   25.58
16384                  41.64
32768                  53.00
65536                  75.47
131072                131.85
262144                231.30
524288                433.43
1048576               838.19
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.4.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
hpc-lhr2-03-rdma slots=8
hpc-lhr2-04-rdma slots=8
hpc-lhr2-05-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 32 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      13.52
2                      12.72
4                      12.81
8                      13.32
16                     13.72
32                     14.43
64                     15.49
128                    17.50
256                    20.75
512                    25.17
1024                   33.31
2048                   57.93
4096                  218.39
8192                  309.13
16384                 527.44
32768                 970.90
65536                1905.46
131072               4135.25
262144               7029.87
524288              13822.00
1048576             26251.33
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.4.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
hpc-lhr2-03-rdma slots=8
hpc-lhr2-04-rdma slots=8
hpc-lhr2-05-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 32 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             5.11
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.4.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
hpc-lhr2-03-rdma slots=8
hpc-lhr2-04-rdma slots=8
hpc-lhr2-05-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 32 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       5.71
8                       5.46
16                      5.32
32                      5.88
64                      6.06
128                     8.57
256                     7.97
512                     8.66
1024                   10.10
2048                   12.95
4096                   18.91
8192                   33.67
16384                  54.15
32768                  68.18
65536                  89.90
131072                140.87
262144                273.09
524288                529.96
1048576               985.25
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.4.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
hpc-lhr2-03-rdma slots=16
hpc-lhr2-04-rdma slots=16
hpc-lhr2-05-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 64 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      17.36
2                      17.00
4                      17.20
8                      17.80
16                     18.66
32                     20.12
64                     23.08
128                    30.99
256                    61.80
512                   113.67
1024                  149.15
2048                  317.51
4096                  617.86
8192                 1090.21
16384                2088.20
32768                4761.11
65536                8834.39
131072              16635.78
262144              31429.93
524288              60962.51
1048576            118934.55
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.4.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
hpc-lhr2-03-rdma slots=16
hpc-lhr2-04-rdma slots=16
hpc-lhr2-05-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 64 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             5.83
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.4.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
hpc-lhr2-03-rdma slots=16
hpc-lhr2-04-rdma slots=16
hpc-lhr2-05-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 64 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       8.38
8                       6.27
16                      6.11
32                      6.60
64                      6.83
128                     8.86
256                     9.31
512                    10.18
1024                   12.12
2048                   16.34
4096                   25.46
8192                   48.50
16384                  84.78
32768                  95.92
65536                 125.25
131072                173.20
262144                286.97
524288                557.17
1048576              1109.34
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.4.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
hpc-lhr2-03-rdma slots=32
hpc-lhr2-04-rdma slots=32
hpc-lhr2-05-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 128 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      24.20
2                      23.71
4                      24.33
8                      25.82
16                     29.06
32                     34.36
64                     69.91
128                   252.35
256                   372.49
512                   496.22
1024                  657.38
2048                 1243.98
4096                 2299.59
8192                 5102.53
16384                8759.52
32768               16719.86
65536               32657.78
131072              63548.03
262144             136473.47
524288             268160.25
1048576            539047.91
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.4.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
hpc-lhr2-03-rdma slots=32
hpc-lhr2-04-rdma slots=32
hpc-lhr2-05-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 128 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             6.51
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.4.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
hpc-lhr2-03-rdma slots=32
hpc-lhr2-04-rdma slots=32
hpc-lhr2-05-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 128 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       7.00
8                       6.91
16                      6.76
32                      7.41
64                      7.75
128                    10.61
256                    10.74
512                    12.22
1024                   15.90
2048                   22.65
4096                   40.99
8192                   78.24
16384                 145.80
32768                 159.21
65536                 170.06
131072                251.76
262144                370.86
524288                648.11
1048576              1404.46
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.4.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
hpc-lhr2-03-rdma slots=36
hpc-lhr2-04-rdma slots=36
hpc-lhr2-05-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 144 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      28.96
2                      28.39
4                      29.40
8                      31.11
16                     33.17
32                     39.27
64                     79.39
128                   434.20
256                   542.10
512                   719.55
1024                  917.49
2048                 1730.46
4096                 3006.17
8192                 6133.67
16384               11424.06
32768               21720.77
65536               41949.64
131072              84438.16
262144             169692.81
524288             335940.30
1048576            671616.98
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.4.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
hpc-lhr2-03-rdma slots=36
hpc-lhr2-04-rdma slots=36
hpc-lhr2-05-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 144 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
            10.46
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.4.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
hpc-lhr2-03-rdma slots=36
hpc-lhr2-04-rdma slots=36
hpc-lhr2-05-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 144 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                      10.02
8                       9.73
16                     10.05
32                     10.36
64                     17.79
128                    15.87
256                    14.78
512                    15.99
1024                   21.00
2048                   28.53
4096                  277.11
8192                  253.15
16384                 160.00
32768                 159.60
65536                 179.76
131072                272.91
262144                391.83
524288                700.55
1048576              1536.93
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.5.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
hpc-lhr2-04-rdma slots=1
hpc-lhr2-05-rdma slots=1
hpc-lhr2-06-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 5 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       2.55
2                       2.32
4                       2.31
8                       2.31
16                      2.31
32                      2.34
64                      2.41
128                     2.96
256                     3.06
512                     3.76
1024                    3.66
2048                    4.83
4096                   15.30
8192                   21.42
16384                  28.91
32768                  38.99
65536                  63.95
131072                135.98
262144                207.44
524288                385.42
1048576               766.57
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.5.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
hpc-lhr2-04-rdma slots=1
hpc-lhr2-05-rdma slots=1
hpc-lhr2-06-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 5 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             4.65
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.5.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
hpc-lhr2-04-rdma slots=1
hpc-lhr2-05-rdma slots=1
hpc-lhr2-06-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 5 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       4.13
8                       3.88
16                      3.81
32                      3.87
64                      3.98
128                     5.16
256                     5.37
512                     5.74
1024                    6.45
2048                    7.92
4096                   11.20
8192                   17.31
16384                  31.47
32768                  41.41
65536                  58.18
131072                 96.19
262144                162.24
524288                284.25
1048576               593.67
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.5.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
hpc-lhr2-03-rdma slots=2
hpc-lhr2-04-rdma slots=2
hpc-lhr2-05-rdma slots=2
hpc-lhr2-06-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 10 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       4.23
2                       3.91
4                       3.87
8                       3.86
16                      3.86
32                      4.05
64                      4.37
128                     5.33
256                     5.56
512                     6.17
1024                    7.28
2048                    9.02
4096                   37.08
8192                   54.43
16384                  73.38
32768                 117.68
65536                 193.53
131072                385.48
262144                544.35
524288               1083.69
1048576              2244.68
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.5.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
hpc-lhr2-03-rdma slots=2
hpc-lhr2-04-rdma slots=2
hpc-lhr2-05-rdma slots=2
hpc-lhr2-06-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 10 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             6.13
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.5.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
hpc-lhr2-03-rdma slots=2
hpc-lhr2-04-rdma slots=2
hpc-lhr2-05-rdma slots=2
hpc-lhr2-06-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 10 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       5.37
8                       5.19
16                      5.11
32                      5.40
64                      5.75
128                     7.40
256                     7.69
512                     8.17
1024                    9.24
2048                   11.22
4096                   15.31
8192                   24.17
16384                  42.02
32768                  54.26
65536                  78.80
131072                126.98
262144                222.54
524288                405.98
1048576               802.26
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.5.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
hpc-lhr2-03-rdma slots=4
hpc-lhr2-04-rdma slots=4
hpc-lhr2-05-rdma slots=4
hpc-lhr2-06-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 20 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      13.53
2                      12.72
4                      12.74
8                      13.10
16                     13.48
32                     13.98
64                     14.65
128                    15.61
256                    11.13
512                    12.87
1024                   16.14
2048                   21.14
4096                   91.89
8192                  139.83
16384                 194.96
32768                 303.36
65536                 588.37
131072               1316.24
262144               2580.64
524288               4538.92
1048576              8787.85
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.5.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
hpc-lhr2-03-rdma slots=4
hpc-lhr2-04-rdma slots=4
hpc-lhr2-05-rdma slots=4
hpc-lhr2-06-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 20 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             7.10
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.5.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
hpc-lhr2-03-rdma slots=4
hpc-lhr2-04-rdma slots=4
hpc-lhr2-05-rdma slots=4
hpc-lhr2-06-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 20 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       6.00
8                       5.73
16                      5.66
32                      6.61
64                      6.55
128                     8.21
256                     8.47
512                     9.07
1024                   10.30
2048                   12.39
4096                   17.23
8192                   28.15
16384                  49.56
32768                  61.43
65536                  83.92
131072                135.53
262144                242.21
524288                465.02
1048576               866.78
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.5.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
hpc-lhr2-03-rdma slots=8
hpc-lhr2-04-rdma slots=8
hpc-lhr2-05-rdma slots=8
hpc-lhr2-06-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 40 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      16.67
2                      16.09
4                      16.21
8                      16.67
16                     17.19
32                     18.13
64                     19.39
128                    22.12
256                    25.56
512                    34.05
1024                   53.61
2048                  147.39
4096                  293.55
8192                  394.15
16384                 627.73
32768                1171.31
65536                2381.78
131072               5914.13
262144               9486.32
524288              17930.56
1048576             33919.54
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.5.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
hpc-lhr2-03-rdma slots=8
hpc-lhr2-04-rdma slots=8
hpc-lhr2-05-rdma slots=8
hpc-lhr2-06-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 40 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             8.00
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.5.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
hpc-lhr2-03-rdma slots=8
hpc-lhr2-04-rdma slots=8
hpc-lhr2-05-rdma slots=8
hpc-lhr2-06-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 40 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       6.67
8                       6.58
16                      6.19
32                      6.85
64                      7.22
128                     9.05
256                     9.35
512                    10.03
1024                   11.53
2048                   14.57
4096                   21.14
8192                   56.87
16384                  68.71
32768                  80.10
65536                 105.08
131072                155.53
262144                287.15
524288                626.39
1048576              1861.32
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.5.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
hpc-lhr2-03-rdma slots=16
hpc-lhr2-04-rdma slots=16
hpc-lhr2-05-rdma slots=16
hpc-lhr2-06-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 80 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      21.50
2                      20.70
4                      21.09
8                      22.24
16                     23.01
32                     24.66
64                     28.49
128                    38.27
256                    93.44
512                   166.50
1024                  254.88
2048                  375.48
4096                  752.30
8192                 1434.51
16384                2821.54
32768                6410.15
65536               11685.48
131072              22203.01
262144              40068.03
524288              77097.31
1048576            150709.70
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.5.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
hpc-lhr2-03-rdma slots=16
hpc-lhr2-04-rdma slots=16
hpc-lhr2-05-rdma slots=16
hpc-lhr2-06-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 80 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             8.85
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.5.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
hpc-lhr2-03-rdma slots=16
hpc-lhr2-04-rdma slots=16
hpc-lhr2-05-rdma slots=16
hpc-lhr2-06-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 80 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       7.38
8                       6.94
16                      6.79
32                      7.83
64                      7.96
128                     9.90
256                    10.39
512                    11.30
1024                   13.57
2048                   17.75
4096                   59.73
8192                  150.65
16384                 132.65
32768                 111.06
65536                 141.66
131072                196.78
262144                310.83
524288                548.91
1048576              1167.44
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.5.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
hpc-lhr2-03-rdma slots=32
hpc-lhr2-04-rdma slots=32
hpc-lhr2-05-rdma slots=32
hpc-lhr2-06-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 160 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      30.41
2                      29.35
4                      30.34
8                      31.94
16                     34.58
32                     40.83
64                     75.89
128                   387.52
256                   544.34
512                   716.17
1024                  932.95
2048                 3421.81
4096                 2718.52
8192                 8442.71
16384               12161.75
32768               22710.33
65536               43608.30
131072             105202.58
262144             177149.60
524288             372423.36
1048576            729508.89
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.5.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
hpc-lhr2-03-rdma slots=32
hpc-lhr2-04-rdma slots=32
hpc-lhr2-05-rdma slots=32
hpc-lhr2-06-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 160 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             9.84
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.5.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
hpc-lhr2-03-rdma slots=32
hpc-lhr2-04-rdma slots=32
hpc-lhr2-05-rdma slots=32
hpc-lhr2-06-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 160 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       8.17
8                       7.82
16                      8.04
32                      8.49
64                      9.02
128                    11.35
256                    12.10
512                    13.43
1024                   16.82
2048                   36.15
4096                  115.55
8192                  185.71
16384                 173.99
32768                 179.46
65536                 196.00
131072                278.95
262144                416.55
524288                687.32
1048576              1358.49
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.5.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
hpc-lhr2-03-rdma slots=36
hpc-lhr2-04-rdma slots=36
hpc-lhr2-05-rdma slots=36
hpc-lhr2-06-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 180 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      31.98
2                      31.97
4                      33.13
8                      35.07
16                     38.12
32                     50.96
64                    121.43
128                   268.41
256                   707.83
512                   980.80
1024                 1291.71
2048                 2552.20
4096                 3816.10
8192                 8273.42
16384               15355.52
32768               29599.34
65536               57196.92
131072             114340.22
262144             220552.35
524288             457399.21
1048576            914865.68
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.5.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
hpc-lhr2-03-rdma slots=36
hpc-lhr2-04-rdma slots=36
hpc-lhr2-05-rdma slots=36
hpc-lhr2-06-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 180 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
            10.84
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.5.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
hpc-lhr2-03-rdma slots=36
hpc-lhr2-04-rdma slots=36
hpc-lhr2-05-rdma slots=36
hpc-lhr2-06-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 180 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                      10.57
8                      10.80
16                     18.05
32                     15.21
64                     11.95
128                    15.02
256                    15.44
512                    16.82
1024                   20.17
2048                   26.81
4096                  223.18
8192                  337.46
16384                 195.13
32768                 200.24
65536                 225.63
131072                466.73
262144                640.61
524288                800.06
1048576              1489.80
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.6.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
hpc-lhr2-04-rdma slots=1
hpc-lhr2-05-rdma slots=1
hpc-lhr2-06-rdma slots=1
hpc-lhr2-07-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 6 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       2.84
2                       2.60
4                       2.54
8                       2.53
16                      2.54
32                      2.61
64                      2.67
128                     3.23
256                     3.34
512                     3.61
1024                    4.09
2048                    5.29
4096                   19.14
8192                   27.07
16384                  36.34
32768                  48.98
65536                  81.59
131072                168.25
262144                234.28
524288                434.04
1048576               844.68
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.6.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
hpc-lhr2-04-rdma slots=1
hpc-lhr2-05-rdma slots=1
hpc-lhr2-06-rdma slots=1
hpc-lhr2-07-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 6 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             4.65
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.6.1.out
cat machinefile
hpc-lhr2-02-rdma slots=1
hpc-lhr2-03-rdma slots=1
hpc-lhr2-04-rdma slots=1
hpc-lhr2-05-rdma slots=1
hpc-lhr2-06-rdma slots=1
hpc-lhr2-07-rdma slots=1
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 6 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       5.00
8                       4.80
16                      4.68
32                      4.65
64                      4.85
128                     6.20
256                     6.49
512                     6.86
1024                    7.66
2048                    9.25
4096                   12.66
8192                   19.10
16384                  34.04
32768                  46.94
65536                  63.98
131072                106.06
262144                174.72
524288                300.47
1048576               609.60
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.6.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
hpc-lhr2-03-rdma slots=2
hpc-lhr2-04-rdma slots=2
hpc-lhr2-05-rdma slots=2
hpc-lhr2-06-rdma slots=2
hpc-lhr2-07-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 12 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                       4.99
2                       4.46
4                       5.00
8                       4.58
16                      4.40
32                      4.49
64                      5.00
128                     6.06
256                     6.40
512                     7.23
1024                    8.71
2048                   10.61
4096                   47.17
8192                   69.43
16384                  92.62
32768                 149.14
65536                 243.24
131072                477.43
262144                662.64
524288               1337.44
1048576              2808.45
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.6.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
hpc-lhr2-03-rdma slots=2
hpc-lhr2-04-rdma slots=2
hpc-lhr2-05-rdma slots=2
hpc-lhr2-06-rdma slots=2
hpc-lhr2-07-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 12 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             6.14
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.6.2.out
cat machinefile
hpc-lhr2-02-rdma slots=2
hpc-lhr2-03-rdma slots=2
hpc-lhr2-04-rdma slots=2
hpc-lhr2-05-rdma slots=2
hpc-lhr2-06-rdma slots=2
hpc-lhr2-07-rdma slots=2
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 12 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       6.04
8                       5.97
16                      5.73
32                      6.05
64                      6.43
128                     8.22
256                     8.61
512                     9.16
1024                   10.34
2048                   12.53
4096                   17.17
8192                   26.64
16384                  48.31
32768                  60.53
65536                  86.28
131072                137.74
262144                233.29
524288                420.33
1048576               817.82
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.6.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
hpc-lhr2-03-rdma slots=4
hpc-lhr2-04-rdma slots=4
hpc-lhr2-05-rdma slots=4
hpc-lhr2-06-rdma slots=4
hpc-lhr2-07-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 24 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      13.86
2                      12.98
4                      12.92
8                      13.38
16                     14.18
32                     14.43
64                     15.24
128                    16.41
256                    13.10
512                    15.59
1024                   19.73
2048                   26.23
4096                  115.50
8192                  175.91
16384                 245.88
32768                 383.72
65536                 801.15
131072               1781.97
262144               3627.75
524288               5961.25
1048576             10739.69
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.6.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
hpc-lhr2-03-rdma slots=4
hpc-lhr2-04-rdma slots=4
hpc-lhr2-05-rdma slots=4
hpc-lhr2-06-rdma slots=4
hpc-lhr2-07-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 24 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             7.15
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.6.4.out
cat machinefile
hpc-lhr2-02-rdma slots=4
hpc-lhr2-03-rdma slots=4
hpc-lhr2-04-rdma slots=4
hpc-lhr2-05-rdma slots=4
hpc-lhr2-06-rdma slots=4
hpc-lhr2-07-rdma slots=4
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 24 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       6.58
8                       6.50
16                      6.18
32                      6.75
64                      7.16
128                     8.84
256                     9.19
512                     9.78
1024                   11.14
2048                   13.57
4096                   18.62
8192                   31.49
16384                  56.26
32768                  69.45
65536                  91.27
131072                144.25
262144                261.02
524288                480.03
1048576               958.59
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.6.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
hpc-lhr2-03-rdma slots=8
hpc-lhr2-04-rdma slots=8
hpc-lhr2-05-rdma slots=8
hpc-lhr2-06-rdma slots=8
hpc-lhr2-07-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 48 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      17.40
2                      16.20
4                      16.63
8                      17.53
16                     17.95
32                     19.03
64                     20.56
128                    24.53
256                    33.52
512                    46.07
1024                   76.80
2048                  136.02
4096                  350.21
8192                  480.82
16384                 740.86
32768                1416.00
65536                3869.78
131072               7582.45
262144              11382.26
524288              22799.19
1048576             41544.13
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.6.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
hpc-lhr2-03-rdma slots=8
hpc-lhr2-04-rdma slots=8
hpc-lhr2-05-rdma slots=8
hpc-lhr2-06-rdma slots=8
hpc-lhr2-07-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 48 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             7.99
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.6.8.out
cat machinefile
hpc-lhr2-02-rdma slots=8
hpc-lhr2-03-rdma slots=8
hpc-lhr2-04-rdma slots=8
hpc-lhr2-05-rdma slots=8
hpc-lhr2-06-rdma slots=8
hpc-lhr2-07-rdma slots=8
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 48 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       7.19
8                       6.89
16                      6.66
32                      7.30
64                      7.72
128                     9.66
256                    10.08
512                    10.79
1024                   12.57
2048                   15.54
4096                   22.02
8192                   37.14
16384                  74.86
32768                  87.39
65536                 115.03
131072                163.00
262144                279.34
524288                535.71
1048576              1071.61
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.6.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
hpc-lhr2-03-rdma slots=16
hpc-lhr2-04-rdma slots=16
hpc-lhr2-05-rdma slots=16
hpc-lhr2-06-rdma slots=16
hpc-lhr2-07-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 96 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      22.22
2                      22.10
4                      22.41
8                      23.38
16                     24.78
32                     26.75
64                     32.29
128                    46.30
256                   128.71
512                   197.34
1024                  285.76
2048                  551.58
4096                  911.16
8192                 1903.91
16384                3993.78
32768                7978.29
65536               14606.76
131072              30452.41
262144              49049.51
524288              94741.58
1048576            184262.41
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.6.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
hpc-lhr2-03-rdma slots=16
hpc-lhr2-04-rdma slots=16
hpc-lhr2-05-rdma slots=16
hpc-lhr2-06-rdma slots=16
hpc-lhr2-07-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 96 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             8.85
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.6.16.out
cat machinefile
hpc-lhr2-02-rdma slots=16
hpc-lhr2-03-rdma slots=16
hpc-lhr2-04-rdma slots=16
hpc-lhr2-05-rdma slots=16
hpc-lhr2-06-rdma slots=16
hpc-lhr2-07-rdma slots=16
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 96 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       7.91
8                       7.71
16                      7.33
32                     10.14
64                      8.49
128                    10.61
256                    11.25
512                    12.22
1024                   14.38
2048                   18.33
4096                   27.31
8192                   54.73
16384                 116.82
32768                 125.03
65536                 151.17
131072                217.21
262144                328.30
524288                562.90
1048576              1152.78
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.6.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
hpc-lhr2-03-rdma slots=32
hpc-lhr2-04-rdma slots=32
hpc-lhr2-05-rdma slots=32
hpc-lhr2-06-rdma slots=32
hpc-lhr2-07-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 192 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      31.42
2                      31.85
4                      32.74
8                      35.25
16                     38.86
32                     52.33
64                    154.33
128                   297.64
256                   672.92
512                   934.64
1024                 1239.09
2048                 2151.92
4096                 4026.64
8192                 8328.72
16384               16514.98
32768               31187.62
65536               62657.60
131072             127054.39
262144             216374.16
524288             445290.17
1048576            915767.36
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.6.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
hpc-lhr2-03-rdma slots=32
hpc-lhr2-04-rdma slots=32
hpc-lhr2-05-rdma slots=32
hpc-lhr2-06-rdma slots=32
hpc-lhr2-07-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 192 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
             9.84
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.6.32.out
cat machinefile
hpc-lhr2-02-rdma slots=32
hpc-lhr2-03-rdma slots=32
hpc-lhr2-04-rdma slots=32
hpc-lhr2-05-rdma slots=32
hpc-lhr2-06-rdma slots=32
hpc-lhr2-07-rdma slots=32
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 192 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                       9.66
8                       8.50
16                      8.24
32                      9.04
64                      9.65
128                    12.17
256                    12.88
512                    14.05
1024                   17.37
2048                   23.69
4096                   49.12
8192                  163.62
16384                 185.49
32768                 207.41
65536                 225.98
131072                314.95
262144                467.37
524288                729.00
1048576              1422.27
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_alltoall.6.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
hpc-lhr2-03-rdma slots=36
hpc-lhr2-04-rdma slots=36
hpc-lhr2-05-rdma slots=36
hpc-lhr2-06-rdma slots=36
hpc-lhr2-07-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 216 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.4.0
# Size       Avg Latency(us)
1                      35.00
2                      34.92
4                      36.30
8                      38.75
16                     45.20
32                     58.69
64                    159.16
128                   397.92
256                   933.77
512                  1268.69
1024                 1675.09
2048                 2785.90
4096                 5296.57
8192                10834.52
16384               21179.31
32768               40079.59
65536               82718.80
131072             168227.93
262144             277107.08
524288             590966.54
1048576           1232822.68
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000 | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_barrier.6.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
hpc-lhr2-03-rdma slots=36
hpc-lhr2-04-rdma slots=36
hpc-lhr2-05-rdma slots=36
hpc-lhr2-06-rdma slots=36
hpc-lhr2-07-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 216 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_barrier -i 400000

# OSU MPI Barrier Latency Test v5.4.0
# Avg Latency(us)
            11.01
hpchub_mpirun /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce | tee -a ../../../runs/run/osu/opc-6-nodes-36-cores/Oracle/2019-01-11_13:29:45/osu_allreduce.6.36.out
cat machinefile
hpc-lhr2-02-rdma slots=36
hpc-lhr2-03-rdma slots=36
hpc-lhr2-04-rdma slots=36
hpc-lhr2-05-rdma slots=36
hpc-lhr2-06-rdma slots=36
hpc-lhr2-07-rdma slots=36
mpirun -mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -np 216 -machinefile machinefile --map-by socket:pe=1 --bind-to core  /home/opc/nfs-share/hpchub_benchmark/tests/osu/osu-micro-benchmarks-5.4/mpi/collective/osu_allreduce

# OSU MPI Allreduce Latency Test v5.4.0
# Size       Avg Latency(us)
4                      12.22
8                      11.12
16                     11.17
32                     11.81
64                     12.42
128                    15.54
256                    16.35
512                    17.73
1024                   22.54
2048                   27.94
4096                  101.46
8192                  263.65
16384                 222.12
32768                 235.40
65536                 247.78
131072                323.78
262144                506.28
524288                791.26
1048576              1846.41
